Question:-1
Question-11. The value of correlation coefficient will always be:
A) between 0 and 1 B) greater than -1
C) between -1 and 1 D) between 0 and -1
Between -1 to 1

2. Which of the following cannot be used for dimensionality reduction?
A) Lasso Regularisation B) PCA
C) Recursive feature elimination D) Ridge Regularisation
Recursive feature elimination


3. Which of the following is not a kernel in Support Vector Machines?
A) linear B) Radial Basis Function
C) hyperplane D) polynomial
Hyperplan


4. Amongst the following, which one is least suitable for a dataset having non-linear decision boundaries?
A) Logistic Regression B) Naïve Bayes Classifier
C) Decision Tree Classifier D) Support Vector Classifier
Logistic Regresstion


5. In a Linear Regression problem, ‘X’ is independent variable and ‘Y’ is dependent variable, where ‘X’ represents
weight in pounds. If you convert the unit of ‘X’ to kilograms, then new coefficient of ‘X’ will be?
(1 kilogram = 2.205 pounds)
A) 2.205 × old coefficient of ‘X’ B) same as old coefficient of ‘X’
C) old coefficient of ‘X’ ÷ 2.205 D) Cannot be determined
same as od coefficenet of "X"


6. As we increase the number of estimators in ADABOOST Classifier, what happens to the accuracy of the model?
A) remains same B) increases
C) decreases D) none of the above
increase 

7. Which of the following is not an advantage of using random forest instead of decision trees?
A) Random Forests reduce overfitting
B) Random Forests explains more variance in data then decision trees
C) Random Forests are easy to interpret
D) Random Forests provide a reliable feature importance estimate
Random Forests provide a reliable feature importance estimate


8. Which of the following are correct about Principal Components?
A) Principal Components are calculated using supervised learning techniques
B) Principal Components are calculated using unsupervised learning techniques
C) Principal Components are linear combinations of Linear Variables.
D) All of the above
Principal Components are calculated using supervised learning techniques


9. Which of the following are applications of clustering?
A) Identifying developed, developing and under-developed countries on the basis of factors like GDP, poverty
index, employment rate, population and living index
B) Identifying loan defaulters in a bank on the basis of previous years’ data of loan accounts.
C) Identifying spam or ham emails
D) Identifying different segments of disease based on BMI, blood pressure, cholesterol, blood sugar levels.
Identifying spam or ham emails

10. Which of the following is(are) hyper parameters of a decision tree?
A) max_depth B) max_features
C) n_estimators D) min_samples_leaf
max_depth


Question:11-What are outliers? Explain the Inter Quartile Range(IQR) method for outlier detection 
Sometimes a dataset can contain extreme values that are outside the range of what is expected and unlike the other data.
These are called outliers
IQR is used to measure variability by dividing a data set into quartiles. The data is sorted in ascending order and split into 4 equal parts. 
Q1, Q2, Q3 called first, 
second and third quartiles are the values which separate the 4 equal part
Q1 represents the 25th percentile of the data.
Q2 represents the 50th percentile of the data.
Q3 represents the 75th percentile of the data.
If a dataset has 2n / 2n+1 data points, then
Q1 = median of the dataset.
Q2 = median of n smallest data points.
Q3 = median of n highest data points.

IQR is the range between the first and the third quartiles namely Q1 and Q3: IQR = Q3 – Q1. 
The data points which fall below Q1 – 1.5 IQR or above Q3 + 1.5 IQR are outliers

Question:12-What is the primary difference between bagging and boosting algorithms?
Differences Between Bagging and Boosting –

S.NO	BAGGING						BOOSTING
1.	Simplest way of combining predictions that
	belong to the same type.			A way of combining predictions that
	belong to the different types.
2.	Aim to decrease variance, not bias.		Aim to decrease bias, not variance.
3.	Each model receives equal weight.		Models are weighted according to their performance.
4.	Each model is built independently.		New models are influenced
	by performance of previously built models.
5.	Different training data subsets are randomly 
	drawn with replacement from the entire 
	training dataset.				Every new subsets contains the elements that were misclassified by previous models.
6.	Bagging tries to solve over-fitting problem.	Boosting tries to reduce bias.
7.	If the classifier is unstable (high variance), 
	then apply bagging.				If the classifier is stable and simple (high bias) the apply boosting.
8.	Random forest.	Gradient boosting.



Question:13-What is adjusted R2 in logistic regression. How is it calculated?
An R-squared result of 70 to 100 indicates that a given portfolio closely tracks the stock index in question, while a score between 0 and 40 indicates a very low 
correlation with the index. Higher R-squared values also indicate the reliability of beta readings. 
Beta measures the volatility of a security or a portfolio.
While R-squared can return a figure that indicates a level of correlation with an index, it has certain limitations when it comes to measuring the impact of 
independent variables on the correlation. This is where adjusted R-squared is useful in measuring correlation.

Question-14:-What is the difference between standardisation and normalisation
Normalization/standardization are designed to achieve a similar goal, which is to create features that have similar ranges to each other and
 widely used in data analysis to help the programmer to get some clue out of the raw data.
In Algebra, Normalization seems to refer to the dividing of a vector by its length and it transforms your data into a range between 0 and 1
And in statistics, Standardization seems to refer to the subtraction of the mean and then dividing by its SD (standard deviation). Standardization transforms your 
data such that the resulting distribution has a mean of 0 and a standard deviation of 1.

Question-15:-. What is cross-validation? Describe one advantage and one disadvantage of using cross-validation
cross_validation is a resampling procedure used to evaluate machine learning models on a limited data sample.
The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold 
cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.
Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in
order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.
It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than 
other methods, such as a simple train/test split.
Advantages of cross-validation:

1-More accurate estimate of out-of-sample accuracy.
2-More “efficient” use of data as every observation is used for both training and testing

Disadvantages of Cross Validation

1. Increases Training Time: Cross Validation drastically increases the training time. Earlier you had to train your model only on one training set, 
but with Cross Validation you have to train your model on multiple training sets. 
2. Needs Expensive Computation: Cross Validation is computationally very expensive in terms of processing power required.
